\chapter{Conclusiones}
\label{ch:conclusiones}

En este proyecto hemos visto un uso muy interesante para las tecnologías Big Data. En principio, cuando pensamos en Big Data, pensamos en una gran cantidad de datos como una red de sensores o datos de usuarios en internet que son procesados para realizar análisis estadísticos, por ejemplo. Este trabajo muestra las virtudes del framework Spark como herramienta para ejecutar trabajos de forma paralela. Hemos demostrado con un ejemplo práctico cómo Spark puede soportar una arquitectura de objetos distribuidos para solucionar problemas en el ámbito científico. 

Este modelo de computación distribuida supone múltiples ventajas con respecto a utilizar otras herramientas como MPI u OpenMP. La principal ventaja es su sencillez. El framework Spark facilita la gestión del clúster de computación y se encarga de la repartición de datos y la distribución de la carga computacional. Esto permite a los desarrolladores centrarse en el algoritmo concreto y no en los detalles específicos de trabajar en un entorno distribuido. Además, como el clúster y la implementación están desacopladas, se pueden mejorar y modificar de forma aislada, pudiendo aprovechar nuevas versiones de Spark y hardware más potente sin hacer ninguna o pocas modificaciones sobre la implementación.\\

Otro logro importante es la integración con Pyomo. Aunque se deben mejorar algunas cuestiones menores, el nuevo módulo de trabajos con Spark se integra perfectamente con el resto del proyecto, funcionando con sólo modificar una opción de ejecución. Esto abre las puertas a, en un futuro, añadir esta funcionalidad a la versión estable de Pyomo para que se pueda utilizar libremente y, al ser un proyecto de código abierto, otros grupos de investigación podrán añadir mejoras al mismo.\\

En cuanto al rendimiento, los valores de escalabilidad prometen ofrecer un buen rendimiento dados un hardware y problemas de entrada adecuados. Podemos observar una mejora superior al 0,5x pasando de 1 a 2 hilos de ejecución. En las pruebas que se pudieron ejecutar, no es posible mejorar estos números con más hilos, probablemente por los problemas ejecutados y la máquina de pruebas. Sin embargo, con alguna optimización a mayores, es posible que consigamos una escalabilidad casi perfecta con respecto al número de nodos de procesamiento. Esto implicaría tener una implementación muy eficiente que permitiría abordar grandes problemas estocásticos en poco tiempo haciendo uso de un clúster de computación adecuado.

\section{Lecciones aprendidas}

El desarrollo de este trabajo ha requerido cambios en la planificación, llegando a ser necesario un mes extra de trabajo. Incluso con este tiempo extra, no ha sido posible centrarse en algunos puntos puntos de interés del trabajo, principalmente el periodo de optimización en el que tendría que aprender y aprovechar las características más concretas de Spark. \\

En un proyecto de esta complejidad y duración, la importancia de la gestión se hace más aparente. Desde la realización del anteproyecto, la planificación temporal del proyecto se realizó sin utilizar ningún método de estimación ni datos concretos. A pesar de tener que realizar un proyecto utilizando tecnologías desconocidas (tanto Python como Spark), condiciones muy relevantes para considerar un ciclo en cascada como inadecuado, se decidió planificar el proyecto en cascada porque era más sencillo organizar el tiempo. Se esperaba que, en el caso de retrasarse en una tarea, simplemente se trabajaría más horas y eso sería suficiente. Después de los primeros retrasos sí se modifica el ciclo de vida tomando en consideración estas limitaciones, pero esto es a costa de eliminar la fase de diseño. Para la solución que se implementó finalmente, el diseño no es una fase demasiado importante y creo que se ejecutó satisfactoriamente.\\

El otro pilar importante de este proyecto es Spark. Adaptar una arquitectura orientada a objetos para funcionar en Spark no es una solución óptima, como podemos comprobar por la cantidad de errores encontrados así como los números de rendimiento finales. Viendo la documentación de Spark, está claro que el caso de uso óptimo es el transformar datos estáticos y ejecutar operaciones sobre ellos. En nuestro caso estamos paralelizando una jerarquía completa de objetos, con multitud de dependencias y estos objetos incluso interactúan con archivos y programas externos. El simple hecho de que esta implementación funcione correctamente demuestra la versatilidad de este framework.\\

Cuando se encuentran problemas en el código que necesitan múltiples horas de pruebas y búsqueda de errores, es muy útil hacer un seguimiento por escrito de las tareas realizadas. En este caso se utilizaron múltiples herramientas que facilitaron en gran medida la realización del proyecto así como de la memoria final:

\begin{itemize}
    \item \textbf{Trello: } Es una herramienta web que permite gestionar paneles con listas de tareas. Con esto es sencillo hacer un seguimiento de las tareas pendientes, las que se están ejecutando actualmente y del trabajo ya finalizado.
    \item \textbf{Git: } Un sistema de control de versiones es crucial cuando se están buscando problemas en el código. Permite modificar secciones enteras de código, introducir pruebas, etc, con la tranquilidad de siempre poder volver a una versión correcta. Además nos da una lista de todos los cambios que hemos hecho con lo que podemos identificar dónde están los errores y cómo se han solucionado. Por último, es muy útil poder cambiar entre versiones del código para realizar múltiples pruebas.
    \item \textbf{Informes de seguimiento: } Al principio del proyecto se decidió realizar documentos de progreso cada poco tiempo. Estos archivos no sólo son útiles para documentar el proyecto, también sirven como puntos de reflexión. Generar un informe de progreso nos fuerza a resumir todo lo hecho hasta ahora y a tener claros los objetivos que se deben perseguir a continuación.
\end{itemize}

\section{Trabajo futuro}

Aunque el desarrollo sufrió múltiples problemas en su realización, el resultado final supone una buena base para una implementación útil de Spark en Pyomo. En su estado actual no provee ninguna ventaja con respecto a las implementaciones existentes pero tiene potencial para ser una alternativa muy eficaz. \\

Partiendo de esta base, hay dos caminos claros a seguir para generar una implementación que ofrezca un buen rendimiento. Podemos mantener la arquitectura orientada a objetos actual y centrarnos en optimizar al gestión de las tareas para que las transformaciones ejecutadas en Spark sean menos y contengan mayor trabajo. Por otro lado, podemos simplificar el trabajo que debe realizarse dentro de Spark para acercarse más al tipo de operaciones que funcionan de mejor en Spark.\\

Todos estos cambios deben apoyarse en una batería de pruebas utilizando un cluster de computación que aproveche al máximo las ventajas que nos ofrece Spark. Los problemas a probar deben tener la complejidad suficiente para visualizar la utilidad de un entorno distribuido.\\

Si se realizan todos estos cambios, el objetivo final, dado que Pyomo es un proyecto abierto, sería realizar un \textit{pull-request} para que cualquiera pueda aprovechar estas ventajas utilizando la versión estable de Pyomo.