Importing model and scenario tree files
Time to import model and scenario tree structure files=0.04 seconds
Scenario Tree Detail
----------------------------------------------------
Tree Nodes:

	Name=AboveAverageNode
	Stage=SecondStage
	Parent=RootNode
	Conditional probability=0.3333
	Children:
		None
	Scenarios:
		AboveAverageScenario

	Name=AverageNode
	Stage=SecondStage
	Parent=RootNode
	Conditional probability=0.3333
	Children:
		None
	Scenarios:
		AverageScenario

	Name=BelowAverageNode
	Stage=SecondStage
	Parent=RootNode
	Conditional probability=0.3333
	Children:
		None
	Scenarios:
		BelowAverageScenario

	Name=RootNode
	Stage=FirstStage
	Parent=None
	Conditional probability=1.0000
	Children:
		AboveAverageNode
		AverageNode
		BelowAverageNode
	Scenarios:
		AboveAverageScenario
		AverageScenario
		BelowAverageScenario

----------------------------------------------------
Stages:
	Name=FirstStage
	Tree Nodes: 
		RootNode
	Variables: 
		 DevotedAcreage : [*] 
	Cost Variable: 
		FirstStageCost

	Name=SecondStage
	Tree Nodes: 
		AboveAverageNode
		AverageNode
		BelowAverageNode
	Variables: 
		 QuantityPurchased : [*] 
		 QuantitySubQuotaSold : [*] 
		 QuantitySuperQuotaSold : [*] 
	Cost Variable: 
		SecondStageCost

----------------------------------------------------
Scenarios:
	Name=AboveAverageScenario
	Probability=0.3333
	Leaf node=AboveAverageNode
	Tree node sequence:
		RootNode
		AboveAverageNode

	Name=AverageScenario
	Probability=0.3333
	Leaf node=AverageNode
	Tree node sequence:
		RootNode
		AverageNode

	Name=BelowAverageScenario
	Probability=0.3333
	Leaf node=BelowAverageNode
	Tree node sequence:
		RootNode
		BelowAverageNode

----------------------------------------------------
Scenario tree is valid!
Node-based instance initialization enabled
Creating instance for scenario=BelowAverageScenario
Node data for scenario=BelowAverageScenario partially loading from file=/media/sf_GitHub/TFG/pyomo/examples/pysp/farmer/nodedata/RootNode.dat
Node data for scenario=BelowAverageScenario partially loading from file=/media/sf_GitHub/TFG/pyomo/examples/pysp/farmer/nodedata/BelowAverageNode.dat
Creating instance for scenario=AverageScenario
Node data for scenario=AverageScenario partially loading from file=/media/sf_GitHub/TFG/pyomo/examples/pysp/farmer/nodedata/RootNode.dat
Node data for scenario=AverageScenario partially loading from file=/media/sf_GitHub/TFG/pyomo/examples/pysp/farmer/nodedata/AverageNode.dat
Creating instance for scenario=AboveAverageScenario
Node data for scenario=AboveAverageScenario partially loading from file=/media/sf_GitHub/TFG/pyomo/examples/pysp/farmer/nodedata/RootNode.dat
Node data for scenario=AboveAverageScenario partially loading from file=/media/sf_GitHub/TFG/pyomo/examples/pysp/farmer/nodedata/AboveAverageNode.dat
Time to construct scenario instances=0.03 seconds
Time link scenario tree with instances=0.00 seconds
Constructing solver manager of type=phspark
Constructing solver type=minos
PH solver configuration: 
   Max iterations=100
   Async mode=False
   Async buffer length=1
   Default global rho=1.0
   Over-relaxation enabled=False
   Sub-problem solver type='minos'
   Keep solver files? False
   Output solver results? False
   Output solver log? False
   Output times? False
2018-06-17 11:07:35 WARN  Utils:66 - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 10.0.2.15 instead (on interface enp0s17)
2018-06-17 11:07:35 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address
2018-06-17 11:07:35 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Trying to add /media/sf_GitHub/TFG/pyomo/pyomo/pysp/phsolverserver.py
Requested 3 servers
Not implemented [phspark::acquire_servers]
Initializing PH

Invoking pre-initialization PH plugins
Broadcasting requests to initialize PH solver servers
Transmitting initialization information to PH solver servers

[PHSpark_Manager]: Requested action initialize
[PHSpark_Manager]: Task id 0
Requested action on queue with   name: 0

[PHSparkWorker]: Requested action initialize
[PHSpark_Worker]: Pre-process scenario values
Received request to initialize PH solver server

Model source: /media/sf_GitHub/TFG/pyomo/examples/pysp/farmer/models/ReferenceModel.py
Scenario Tree source: /media/sf_GitHub/TFG/pyomo/examples/pysp/farmer/nodedata/ScenarioStructure.dat
Solver type: minos
Scenario or bundle name: AboveAverageScenario
Scenario tree random seed: 315635154555649683222721639389
Linearize non-binary penalty terms: 0
PRE INITIALIZATION PHSOLVERSERVER CALLBACK INVOKED ON WORKER: SparkWorker_26063@localhost.localdomain
Constructing solver type=minos
Node-based instance initialization enabled
Creating instance for scenario=AboveAverageScenario
Node data for scenario=AboveAverageScenario partially loading from file=/media/sf_GitHub/TFG/pyomo/examples/pysp/farmer/nodedata/RootNode.dat
Node data for scenario=AboveAverageScenario partially loading from file=/media/sf_GitHub/TFG/pyomo/examples/pysp/farmer/nodedata/AboveAverageNode.dat
Invoking post-instance-creation PHSolverServer plugins
POST INSTANCE CREATION PHSOLVERSERVER CALLBACK INVOKED ON WORKER: SparkWorker_26063@localhost.localdomain
Invoking post-initialization PHSolverServer plugins
POST INITIALIZATION PHSOLVERSERVER CALLBACK INVOKED ON WORKER: SparkWorker_26063@localhost.localdomain
[PHSpark_Worker]: Post-process scenario values
RootNode - x: {'DevotedAcreage:$SUGAR_BEETS': 0.0, 'DevotedAcreage:$CORN': 0.0, 'DevotedAcreage:$WHEAT': 0.0}
AboveAverageNode - x: {}
AboveAverageScenario - w: {'RootNode': {'DevotedAcreage:$SUGAR_BEETS': 0.0, 'DevotedAcreage:$CORN': 0.0, 'DevotedAcreage:$WHEAT': 0.0}, 'AboveAverageNode': {}}

[PHSpark_Manager]: Requested action initialize
[PHSpark_Manager]: Task id 1
Requested action on queue with   name: 1

[PHSparkWorker]: Requested action initialize
[PHSpark_Worker]: Pre-process scenario values
Received request to initialize PH solver server

Model source: /media/sf_GitHub/TFG/pyomo/examples/pysp/farmer/models/ReferenceModel.py
Scenario Tree source: /media/sf_GitHub/TFG/pyomo/examples/pysp/farmer/nodedata/ScenarioStructure.dat
Solver type: minos
Scenario or bundle name: AverageScenario
Scenario tree random seed: 315635154555649683222721639389
Linearize non-binary penalty terms: 0
PRE INITIALIZATION PHSOLVERSERVER CALLBACK INVOKED ON WORKER: SparkWorker_26063@localhost.localdomain
Constructing solver type=minos
Node-based instance initialization enabled
Creating instance for scenario=AverageScenario
Node data for scenario=AverageScenario partially loading from file=/media/sf_GitHub/TFG/pyomo/examples/pysp/farmer/nodedata/RootNode.dat
Node data for scenario=AverageScenario partially loading from file=/media/sf_GitHub/TFG/pyomo/examples/pysp/farmer/nodedata/AverageNode.dat
Invoking post-instance-creation PHSolverServer plugins
POST INSTANCE CREATION PHSOLVERSERVER CALLBACK INVOKED ON WORKER: SparkWorker_26063@localhost.localdomain
Invoking post-initialization PHSolverServer plugins
POST INITIALIZATION PHSOLVERSERVER CALLBACK INVOKED ON WORKER: SparkWorker_26063@localhost.localdomain
[PHSpark_Worker]: Post-process scenario values
RootNode - x: {'DevotedAcreage:$SUGAR_BEETS': 0.0, 'DevotedAcreage:$CORN': 0.0, 'DevotedAcreage:$WHEAT': 0.0}
AverageNode - x: {}
AverageScenario - w: {'RootNode': {'DevotedAcreage:$SUGAR_BEETS': 0.0, 'DevotedAcreage:$CORN': 0.0, 'DevotedAcreage:$WHEAT': 0.0}, 'AverageNode': {}}

[PHSpark_Manager]: Requested action initialize
[PHSpark_Manager]: Task id 2
Requested action on queue with   name: 2

[PHSparkWorker]: Requested action initialize
[PHSpark_Worker]: Pre-process scenario values
Received request to initialize PH solver server

Model source: /media/sf_GitHub/TFG/pyomo/examples/pysp/farmer/models/ReferenceModel.py
Scenario Tree source: /media/sf_GitHub/TFG/pyomo/examples/pysp/farmer/nodedata/ScenarioStructure.dat
Solver type: minos
Scenario or bundle name: BelowAverageScenario
Scenario tree random seed: 315635154555649683222721639389
Linearize non-binary penalty terms: 0
PRE INITIALIZATION PHSOLVERSERVER CALLBACK INVOKED ON WORKER: SparkWorker_26063@localhost.localdomain
Constructing solver type=minos
Node-based instance initialization enabled
Creating instance for scenario=BelowAverageScenario
Node data for scenario=BelowAverageScenario partially loading from file=/media/sf_GitHub/TFG/pyomo/examples/pysp/farmer/nodedata/RootNode.dat
Node data for scenario=BelowAverageScenario partially loading from file=/media/sf_GitHub/TFG/pyomo/examples/pysp/farmer/nodedata/BelowAverageNode.dat
Invoking post-instance-creation PHSolverServer plugins
POST INSTANCE CREATION PHSOLVERSERVER CALLBACK INVOKED ON WORKER: SparkWorker_26063@localhost.localdomain
Invoking post-initialization PHSolverServer plugins
POST INITIALIZATION PHSOLVERSERVER CALLBACK INVOKED ON WORKER: SparkWorker_26063@localhost.localdomain
[PHSpark_Worker]: Post-process scenario values
RootNode - x: {'DevotedAcreage:$SUGAR_BEETS': 0.0, 'DevotedAcreage:$CORN': 0.0, 'DevotedAcreage:$WHEAT': 0.0}
BelowAverageNode - x: {}
BelowAverageScenario - w: {'RootNode': {'DevotedAcreage:$SUGAR_BEETS': 0.0, 'DevotedAcreage:$CORN': 0.0, 'DevotedAcreage:$WHEAT': 0.0}, 'BelowAverageNode': {}}
PH solver server initialization requests successfully transmitted
Broadcasting requests to collect scenario tree instance data from PH solver servers
Collecting scenario tree data from PH solver servers

[PHSpark_Manager]: Requested action collect_scenario_tree_data
[PHSpark_Manager]: Task id 3
Requested action on queue with   name: 2
Serializing...
Manually imported: <module 'ReferenceModel' from '/media/sf_GitHub/TFG/pyomo/examples/pysp/farmer/models/ReferenceModel.py'>
Inside sys.modules: <module 'ReferenceModel' from '/media/sf_GitHub/TFG/pyomo/examples/pysp/farmer/models/ReferenceModel.py'>
Serializing...
Manually imported: <module 'ReferenceModel' from '/media/sf_GitHub/TFG/pyomo/examples/pysp/farmer/models/ReferenceModel.py'>
Inside sys.modules: <module 'ReferenceModel' from '/media/sf_GitHub/TFG/pyomo/examples/pysp/farmer/models/ReferenceModel.py'>
Serializing...
Manually imported: <module 'ReferenceModel' from '/media/sf_GitHub/TFG/pyomo/examples/pysp/farmer/models/ReferenceModel.py'>
Inside sys.modules: <module 'ReferenceModel' from '/media/sf_GitHub/TFG/pyomo/examples/pysp/farmer/models/ReferenceModel.py'>

[PHSpark_Manager]: Requested action collect_scenario_tree_data
[PHSpark_Manager]: Task id 4
Requested action on queue with   name: 1

[PHSpark_Manager]: Requested action collect_scenario_tree_data
[PHSpark_Manager]: Task id 5
Requested action on queue with   name: 0
Waiting for scenario tree data extraction
[Stage 0:>                                                          (0 + 2) / 2]                                                                                [PHSpark_Manager] Extracting result for task with id: 0
[PHSpark_Manager] Extracting result for task with id: 5
Successfully loaded scenario tree data for scenario=AboveAverageScenario
[PHSpark_Manager] Extracting result for task with id: 1
[PHSpark_Manager] Extracting result for task with id: 4
Successfully loaded scenario tree data for scenario=AverageScenario
[PHSpark_Manager] Extracting result for task with id: 2
[PHSpark_Manager] Extracting result for task with id: 3
Successfully loaded scenario tree data for scenario=BelowAverageScenario
Scenario tree instance data successfully collected
PH successfully created model instances for all scenarios
PH is successfully initialized
Invoking post-initialization PH plugins
Starting PH

Initiating PH iteration=0
------------------------------------------------
Starting PH iteration 0 solves
Synchronizing fixed variable status with PH solver servers
No synchronization was needed for scenario BelowAverageScenario
No synchronization was needed for scenario AverageScenario
No synchronization was needed for scenario AboveAverageScenario
Queuing solve for scenario=BelowAverageScenario

[PHSpark_Manager]: Requested action solve
[PHSpark_Manager]: Task id 6
Requested action on queue with   name: 2
Queuing solve for scenario=AverageScenario

[PHSpark_Manager]: Requested action solve
[PHSpark_Manager]: Task id 7
Requested action on queue with   name: 1
Queuing solve for scenario=AboveAverageScenario

[PHSpark_Manager]: Requested action solve
[PHSpark_Manager]: Task id 8
Requested action on queue with   name: 0
Waiting for scenario sub-problem solves
2018-06-17 11:07:41 WARN  TaskSetManager:66 - Lost task 0.0 in stage 1.0 (TID 2, 10.0.2.15, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/crist/Downloads/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py", line 229, in main
    process()
  File "/home/crist/Downloads/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py", line 224, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/crist/Downloads/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py", line 372, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/media/sf_GitHub/TFG/pyomo/pyomo/solvers/plugins/smanager/phspark.py", line 85, in <lambda>
    self._rddWorkerList = self._rddWorkerList.map(lambda worker: _do_parallel_bulk(worker, task_dict))
  File "/media/sf_GitHub/TFG/pyomo/pyomo/solvers/plugins/smanager/phspark.py", line 78, in _do_parallel_bulk
    worker.process(task)
  File "/home/crist/Downloads/spark-2.3.0-bin-hadoop2.7/work/app-20180617110736-0004/0/phsolverserver.py", line 122, in process
    result = self._solver_server.process(data)
  File "/home/crist/Downloads/spark-2.3.0-bin-hadoop2.7/work/app-20180617110736-0004/0/phsolverserver.py", line 1276, in process
    data.variable_transmission)
  File "/home/crist/Downloads/spark-2.3.0-bin-hadoop2.7/work/app-20180617110736-0004/0/phsolverserver.py", line 595, in solve
    self._preprocess_scenario_instances()
  File "/home/crist/python-venv/pyomo3/lib64/python3.6/site-packages/pyomo/pysp/ph.py", line 966, in _preprocess_scenario_instances
    self._solver_map[scenario_name])
AttributeError: '_PHSolverServer' object has no attribute '_solver_map'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[Stage 1:>                                                          (0 + 2) / 2]2018-06-17 11:07:42 ERROR TaskSetManager:70 - Task 0 in stage 1.0 failed 4 times; aborting job
Revoking PHPyroWorker job assignments

[PHSpark_Manager]: Requested action release
[PHSpark_Manager]: Task id 9
Requested action on queue with   name: 0

[PHSpark_Manager]: Requested action release
[PHSpark_Manager]: Task id 10
Requested action on queue with   name: 1

[PHSpark_Manager]: Requested action release
[PHSpark_Manager]: Task id 11
Requested action on queue with   name: 2
Traceback (most recent call last):
  File "/media/sf_GitHub/TFG/pyomo/pyomo/pysp/phinit.py", line 1354, in <module>
    PH_main()
  File "/media/sf_GitHub/TFG/pyomo/pyomo/pysp/phinit.py", line 1350, in PH_main
Not implemented [phspark::release_servers]
    return main(args=args)
  File "/media/sf_GitHub/TFG/pyomo/pyomo/pysp/phinit.py", line 1345, in main
    traceback=options.traceback)
  File "/media/sf_GitHub/TFG/pyomo/pyomo/pysp/util/misc.py", line 344, in launch_command
    rc = command(options, *cmd_args, **cmd_kwds)
  File "/media/sf_GitHub/TFG/pyomo/pyomo/pysp/phinit.py", line 1284, in exec_runph
    run_ph(options, ph)
  File "/media/sf_GitHub/TFG/pyomo/pyomo/pysp/phinit.py", line 1032, in run_ph
    retval = ph.solve()
  File "/media/sf_GitHub/TFG/pyomo/pyomo/pysp/ph.py", line 4113, in solve
    iter0retval = self.iteration_0_solves()
  File "/media/sf_GitHub/TFG/pyomo/pyomo/pysp/ph.py", line 3463, in iteration_0_solves
    failures = self.solve_subproblems(warmstart=self._iteration_0_has_warmstart)
  File "/media/sf_GitHub/TFG/pyomo/pyomo/pysp/ph.py", line 3368, in solve_subproblems
    bundle_action_handle_map)
  File "/media/sf_GitHub/TFG/pyomo/pyomo/pysp/ph.py", line 3146, in wait_for_and_process_subproblems
    action_handle = self._solver_manager.wait_any()
  File "/media/sf_GitHub/TFG/pyomo/pyomo/opt/parallel/manager.py", line 160, in wait_any
    ah = self._perform_wait_any()
  File "/media/sf_GitHub/TFG/pyomo/pyomo/solvers/plugins/smanager/phspark.py", line 203, in _perform_wait_any
    result_list = self._rddWorkerList.map(lambda pair: pair[1]).collect()
  File "/home/crist/python-venv/pyomo3/lib64/python3.6/site-packages/pyspark/rdd.py", line 824, in collect
    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
  File "/home/crist/python-venv/pyomo3/lib64/python3.6/site-packages/py4j/java_gateway.py", line 1160, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/home/crist/python-venv/pyomo3/lib64/python3.6/site-packages/py4j/protocol.py", line 320, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 8, 10.0.2.15, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/crist/Downloads/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py", line 229, in main
    process()
  File "/home/crist/Downloads/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py", line 224, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/crist/Downloads/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py", line 372, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/media/sf_GitHub/TFG/pyomo/pyomo/solvers/plugins/smanager/phspark.py", line 85, in <lambda>
    self._rddWorkerList = self._rddWorkerList.map(lambda worker: _do_parallel_bulk(worker, task_dict))
  File "/media/sf_GitHub/TFG/pyomo/pyomo/solvers/plugins/smanager/phspark.py", line 78, in _do_parallel_bulk
    worker.process(task)
  File "/home/crist/Downloads/spark-2.3.0-bin-hadoop2.7/work/app-20180617110736-0004/0/phsolverserver.py", line 122, in process
    result = self._solver_server.process(data)
  File "/home/crist/Downloads/spark-2.3.0-bin-hadoop2.7/work/app-20180617110736-0004/0/phsolverserver.py", line 1276, in process
    data.variable_transmission)
  File "/home/crist/Downloads/spark-2.3.0-bin-hadoop2.7/work/app-20180617110736-0004/0/phsolverserver.py", line 595, in solve
    self._preprocess_scenario_instances()
  File "/home/crist/python-venv/pyomo3/lib64/python3.6/site-packages/pyomo/pysp/ph.py", line 966, in _preprocess_scenario_instances
    self._solver_map[scenario_name])
AttributeError: '_PHSolverServer' object has no attribute '_solver_map'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:938)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/crist/Downloads/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py", line 229, in main
    process()
  File "/home/crist/Downloads/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py", line 224, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/crist/Downloads/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py", line 372, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/media/sf_GitHub/TFG/pyomo/pyomo/solvers/plugins/smanager/phspark.py", line 85, in <lambda>
    self._rddWorkerList = self._rddWorkerList.map(lambda worker: _do_parallel_bulk(worker, task_dict))
  File "/media/sf_GitHub/TFG/pyomo/pyomo/solvers/plugins/smanager/phspark.py", line 78, in _do_parallel_bulk
    worker.process(task)
  File "/home/crist/Downloads/spark-2.3.0-bin-hadoop2.7/work/app-20180617110736-0004/0/phsolverserver.py", line 122, in process
    result = self._solver_server.process(data)
  File "/home/crist/Downloads/spark-2.3.0-bin-hadoop2.7/work/app-20180617110736-0004/0/phsolverserver.py", line 1276, in process
    data.variable_transmission)
  File "/home/crist/Downloads/spark-2.3.0-bin-hadoop2.7/work/app-20180617110736-0004/0/phsolverserver.py", line 595, in solve
    self._preprocess_scenario_instances()
  File "/home/crist/python-venv/pyomo3/lib64/python3.6/site-packages/pyomo/pysp/ph.py", line 966, in _preprocess_scenario_instances
    self._solver_map[scenario_name])
AttributeError: '_PHSolverServer' object has no attribute '_solver_map'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

